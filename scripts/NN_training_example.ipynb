{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938ab8b5",
   "metadata": {},
   "source": [
    "Load dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297119a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import NN_Trainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from reaction_class import Reaction as rc\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "path = Path.cwd()\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492bd991",
   "metadata": {},
   "source": [
    "### Generating reaction presence dataframe\n",
    "\n",
    "To prepare the training data we need to determine the reactions present in your training metabolic models. This means that we need generate a list of possible reactions found in your training data, which will serve as the reaction keys. We can then determine for every draft training models which of these reactions are present and create a binary list of reactions presences. We will end up with a binary array with on one axis the different reactions and on the other every model in the training data. \n",
    "\n",
    "We will use the class we build but you can use any module to load metabolic models or extract the reaction sets in another way, the key is to end up with a binary array of reaction presences. If you already have this, this step can be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to training models\n",
    "\n",
    "model_path =  ''\n",
    "\n",
    "#output path training data\n",
    "\n",
    "output_path = ''\n",
    "\n",
    "#list of model-ids of draft-models\n",
    "paths  = os.listdir(model_path)\n",
    "model_ids = []\n",
    "for filename in paths:\n",
    "    model_ids.append(filename[:-5])\n",
    "n_models = len(model_ids)\n",
    "dic = {}\n",
    "rxn = []\n",
    "for file_path, model_id in zip(paths,model_ids):\n",
    "    print(model_id)\n",
    "    model = rc(model = os.path.join(model_path, file_path))\n",
    "    rs = set(model.reactions)\n",
    "    dic[model_id]=rs\n",
    "    \n",
    "    #generate a list of all possible reactions\n",
    "    for i in list(rs):\n",
    "         if i not in rxn:\n",
    "             rxn.append(i)\n",
    "\n",
    "n_reactions = len(rxn)\n",
    "\n",
    "reaction_df=pd.DataFrame(index=rxn, columns=model_ids)\n",
    "for key, value in dic.items():\n",
    "    a = []\n",
    "    for i in rxn:\n",
    "        if i in value:\n",
    "            a.append(1)\n",
    "        else:\n",
    "            a.append(0)\n",
    "    reaction_df[key]=a\n",
    "\n",
    "#saving to pandas file\n",
    "reaction_df.to_csv(output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9000859",
   "metadata": {},
   "source": [
    "### Training the Neural Network\n",
    "\n",
    "The easiest way to train the network requires providing a pandas dataframe where the index are the reaction keys and the columns the different training examples (see above). You can also provide a numpy array and the reaction keys as a separate list. During training the function will automatically generate the training dataset. You can change the number of times each training model is used (nuplo). You can also give a range of deletion percentages (min_for to max_for) which will be removed in equal sized steps based on the number of replicates. There is also optional parameter that can be used to weigh the deletion of certain reactions (del_p). It is also possible to add false reactions (using min_con and max_con), but we do not currently use it and it will not work with the masking of input reactions (as the mask does not differentiate between contamination and real reactions).\n",
    "\n",
    "You can provide labels (the full set of reactions) for the network to try and predict, if no labels are provided the network will asume that your input (the data without deletions) should be what the network tries to predict. \n",
    "\n",
    "You can rely on the default parameters to define the network which we optimised for our usecase, but for optimal perfomance on different datasets, you might want to change the hyperparameters (dropout, batch size), the architecture (nnodes, nlayers) or bias of predicted classes. You can also disable the masking of input positions during loss calculation. Finally you can determine a validation split which will set apart a part of your input data during training and calculate scores after to validate your network.\n",
    "\n",
    "The function will return a class containing a Tensorflow object (the network), the list of reactions which respond to the output nodes (reaction keys) and the modeltype (ModelSEED, BiGG etc.). If save=True you can save these as a .h5 file. \n",
    "\n",
    "Finally you can set history = True to also return the history of training for optimisation purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        PARAMETERS:\n",
    "        ----------\n",
    "        data: DataFrame or array, required\n",
    "            binary array of reactions presences, for DataFrame index is used as rxn_keys\n",
    "            otherwise rxn_keys should be provided\n",
    "        modeltype : string, (currently) required\n",
    "            The modeltype of the training data,\n",
    "        rxn_keys: list, optional\n",
    "            Can be used if data is not a pandas dataframe but a numpy array. Default is None\n",
    "        labels:\n",
    "            User can specify labels, by default input data is used as labels\n",
    "\n",
    "        TRAINING PARAMETERS:\n",
    "        -------\n",
    "        nuplo: int\n",
    "            create duplicates of input data\n",
    "            default=30\n",
    "\n",
    "        The omission and contamination rates will increase linearly from min to max,\n",
    "        with stepsize determined by nuplo\n",
    "        min_for, float\n",
    "            minimum false omssion rate, default = 0.05\n",
    "        max_for, float\n",
    "            maximum false ommision rate, default = 0.55\n",
    "        min_con, float\n",
    "            minimum contanimation introduced, currently not used, default = 0\n",
    "        max_con, float\n",
    "            maximum contamination introduced, currently not used, default = 0\n",
    "        del_p, list\n",
    "            list of probabilities of deletion for reactions\n",
    "        con_p, list\n",
    "            list of probabilities of introduction for reactions\n",
    "\n",
    "        NETWORK PARAMETERS\n",
    "        -------------\n",
    "        nlayers: int, optional\n",
    "            number of hidden layers (layers that are not input or output)\n",
    "            default=1\n",
    "        nnodes: int, optional\n",
    "            number of nodes per layer,\n",
    "            default=256\n",
    "        nepochs: int, optional\n",
    "            how often the network needs to loop over all the data\n",
    "            default=10\n",
    "        b_size: int, optional\n",
    "            batch_size (number of training examples that are simultaneously evaluated)\n",
    "            default=32,\n",
    "        dropout: float, optional\n",
    "            parameter for training that can reduce overfitting\n",
    "            default = 0.1,\n",
    "        bias_0: float, optional\n",
    "            default = 0.3,\n",
    "        maskI: boolean, optional\n",
    "            Determines wether the input positions are masked during loss calculation, default=True\n",
    "            default=True\n",
    "        validation_split: float, optional\n",
    "            Splits the input data in training and validation\n",
    "            default = 0 (no split)\n",
    "\n",
    "        SAVING PARAMETERS:\n",
    "\n",
    "        save: boolean, optional\n",
    "            Whether you want to save the network, default = False\n",
    "        name: string, optional\n",
    "            name of your network, default='noname'\n",
    "        output_path: string,\n",
    "            where output, default=''\n",
    "        return_history: boolean, optional\n",
    "            If you want training history\n",
    "\n",
    "       Returns:\n",
    "        -------------\n",
    "        trainedNN\n",
    "            NN class containing network, rxn_keys and modeltype\n",
    "        history: History(), if history=True\n",
    "            history of training, this can be used to look at the performance during training\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59319dbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load in a small training sample\n",
    "file_path = os.path.join(path.parent,'files', 'NN')\n",
    "data = pd.read_csv(os.path.join(file_path, 'Sample_reaction_presence.csv'), index_col=0)\n",
    "\n",
    "network = NN_Trainer.train(data=data, modeltype='ModelSEED',name='example',output_path=file_path, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfcob",
   "language": "python",
   "name": "tfcob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
